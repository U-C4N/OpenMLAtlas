# ğŸ² 03 - Classical Machine Learning Algorithms

Welcome to the **Classical Algorithms** module of OpenMLAtlas! This is where you'll master the powerful, time-tested algorithms that form the core of traditional machine learning. From decision trees to support vector machines, these methods remain highly effective for many real-world problems.

## ğŸ“ Overview

This module covers advanced classical machine learning algorithms that go beyond basic linear models. You'll learn tree-based methods, support vector machines, clustering techniques, dimensionality reduction, and ensemble methodsâ€”all essential tools in any ML practitioner's toolkit.

## ğŸ§© Module Structure

| Directory | Topic | Description |
|-----------|-------|-------------|
| ğŸŒ³ `tree-based-models/` | **Tree-Based Models** | **Decision trees and their powerful extensions** |
| â†³ `decision-trees/` | Decision Trees | CART, ID3, C4.5 for classification and regression |
| â†³ `random-forest/` | Random Forests | Ensemble of decision trees using bagging |
| â†³ `gradient-boosting/` | Gradient Boosting | Sequential ensemble learning for high performance |
| â†³ `xgboost-lightgbm-catboost/` | Modern Boosting | State-of-the-art gradient boosting implementations |
| ğŸ¯ `svm/` | Support Vector Machines | Maximum margin classifiers, kernels, SVR |
| ğŸ”µ `clustering/` | **Clustering Algorithms** | **Unsupervised grouping and pattern discovery** |
| â†³ `kmeans/` | K-Means Clustering | Centroid-based clustering algorithm |
| â†³ `hierarchical-clustering/` | Hierarchical Clustering | Agglomerative and divisive clustering approaches |
| â†³ `dbscan/` | DBSCAN | Density-based clustering for arbitrary shapes |
| â†³ `gmm/` | Gaussian Mixture Models | Probabilistic clustering with EM algorithm |
| ğŸ“‰ `dimensionality-reduction/` | **Dimensionality Reduction** | **Reducing features while preserving information** |
| â†³ `pca/` | Principal Component Analysis | Linear dimensionality reduction via eigenvectors |
| â†³ `lda/` | Linear Discriminant Analysis | Supervised dimensionality reduction |
| â†³ `manifold-learning-tsne-umap/` | Manifold Learning | t-SNE, UMAP for nonlinear dimensionality reduction |
| ğŸª `ensemble-methods/` | **Ensemble Methods** | **Combining multiple models for better performance** |
| â†³ `bagging/` | Bagging | Bootstrap aggregating to reduce variance |
| â†³ `boosting/` | Boosting | Sequential learning to reduce bias |
| â†³ `stacking/` | Stacking | Meta-learning by combining diverse models |

## ğŸ—ºï¸ Learning Path

We recommend following this order:

1. **tree-based-models/** - Start with tree-based methods
   - **decision-trees/** - Understand the foundation
   - **random-forest/** - Learn ensemble with bagging
   - **gradient-boosting/** - Master sequential ensemble learning
   - **xgboost-lightgbm-catboost/** - Apply modern implementations
2. **svm/** - Learn maximum margin classifiers and kernel methods
3. **clustering/** - Master unsupervised learning
   - **kmeans/** - Start with the most popular clustering algorithm
   - **hierarchical-clustering/** - Learn dendrogram-based approaches
   - **dbscan/** - Understand density-based clustering
   - **gmm/** - Explore probabilistic clustering
4. **dimensionality-reduction/** - Learn to handle high-dimensional data
   - **pca/** - Master the most common technique
   - **lda/** - Understand supervised reduction
   - **manifold-learning-tsne-umap/** - Explore nonlinear methods
5. **ensemble-methods/** - Combine everything you've learned
   - **bagging/** - Reduce variance through averaging
   - **boosting/** - Reduce bias through sequential learning
   - **stacking/** - Build meta-models for optimal performance

However, feel free to jump to specific topics based on your needs!

## ğŸ”‘ What You'll Learn

### ğŸŒ³ Tree-Based Methods
- **Decision Trees**: How to build interpretable models using recursive partitioning
- **Random Forests**: How bagging creates robust, high-performing ensembles
- **Gradient Boosting**: How sequential learning corrects errors iteratively
- **Modern Boosting**: XGBoost, LightGBM, CatBoost for production-grade performance

### ğŸ¯ Support Vector Machines
- **Linear SVM**: Maximum margin classification for linearly separable data
- **Kernel Trick**: Mapping data to higher dimensions without explicit computation
- **Non-linear SVM**: RBF, polynomial, and custom kernels
- **Support Vector Regression**: Extending SVM to regression problems

### ğŸ”µ Clustering Algorithms
- **K-Means**: Centroid-based partitioning and choosing optimal K
- **Hierarchical Clustering**: Building dendrograms and linkage methods
- **DBSCAN**: Density-based clustering for arbitrary-shaped clusters
- **GMM**: Probabilistic clustering with soft assignments

### ğŸ“‰ Dimensionality Reduction
- **PCA**: Variance-preserving linear projection
- **LDA**: Class-separating linear projection
- **t-SNE & UMAP**: Nonlinear manifold learning for visualization and preprocessing

### ğŸª Ensemble Strategies
- **Bagging**: Reducing variance through bootstrap aggregation
- **Boosting**: Reducing bias through adaptive reweighting
- **Stacking**: Combining diverse models with meta-learners

## ğŸ“‹ Prerequisites

Before starting this module, you should be familiar with:
- **Core ML Concepts**: Supervised vs unsupervised learning, model evaluation
- **Linear Models**: Linear and logistic regression, regularization
- **Probability & Statistics**: Distributions, statistical testing
- **Linear Algebra**: Matrix operations, eigenvalues/eigenvectors
- **Python/Scikit-learn**: Basic ML workflows
- Completed **[02-core-ml/](../02-core-ml/)** or equivalent knowledge

## ğŸ¬ Getting Started

1. Ensure you have completed the prerequisites
2. Install required libraries: `pip install numpy pandas matplotlib scikit-learn xgboost lightgbm catboost jupyter`
3. Start with the recommended learning path or jump to a specific topic
4. Work through theory, code examples, and exercises in each subdirectory
5. Compare different algorithms on the same datasets to understand their strengths

## ğŸ“š How to Use This Module

Each subdirectory contains:
- **Theory**: Concept explanations and mathematical foundations (`.md` files)
- **Jupyter Notebooks**: Interactive code examples with visualizations (`.ipynb` files)
- **Comparisons**: Side-by-side algorithm comparisons
- **Exercises**: Practice problems to reinforce your learning
- **Projects**: Hands-on projects to apply what you've learned
- **Resources**: Additional reading materials and references

### ğŸ”§ Working with Jupyter Notebooks

To run the interactive examples:
```bash
# Start Jupyter Notebook
jupyter notebook

# Or use JupyterLab for a better experience
jupyter lab
```

Each notebook includes:
- ğŸ“ Step-by-step algorithm implementations with explanations
- ğŸ“Š Interactive visualizations showing how algorithms work
- ğŸ§ª Real datasets to experiment with different methods
- ğŸ’ª Hands-on exercises comparing algorithm performance
- ğŸ¯ Practical tips for algorithm selection and tuning

## ğŸš€ Next Steps

Once you've mastered classical algorithms, move on to:
- **[04-deep-learning/](../04-deep-learning/)** - Neural networks and deep learning

## ğŸ’¬ Contributing

Found an error? Have a suggestion? Feel free to open an issue or submit a pull request!

## ğŸ“œ License

This project is part of OpenMLAtlas - An open-source machine learning learning resource.

---

âœ¨ **Remember**: These classical algorithms are still widely used in production! They're often faster to train, more interpretable, and require less data than deep learning methods. Master them well!
