# ğŸ¯ 02 - Core Machine Learning

Welcome to the **Core Machine Learning** module of OpenMLAtlas! This is where you begin your journey into the world of machine learning algorithms. Here, you'll learn the fundamental supervised and unsupervised learning techniques that form the backbone of modern ML.

## ğŸ“ Overview

This module covers essential machine learning concepts and algorithms. You'll learn how to build, train, and evaluate ML models, understand their strengths and limitations, and apply them to real-world problems.

## ğŸ§© Module Structure

| Directory | Topic | Description |
|-----------|-------|-------------|
| ğŸ“Š `supervised-vs-unsupervised/` | ML Paradigms | Understanding different types of machine learning approaches |
| ğŸ“ˆ `linear-regression/` | Linear Regression | Predicting continuous values using linear relationships |
| ğŸ“‰ `logistic-regression/` | Logistic Regression | Binary and multiclass classification using logistic functions |
| ğŸ¯ `knn/` | K-Nearest Neighbors | Instance-based learning for classification and regression |
| ğŸ§® `naive-bayes/` | Naive Bayes | Probabilistic classification based on Bayes' theorem |
| ğŸ¨ `feature-engineering/` | Feature Engineering | Creating and selecting features to improve model performance |
| ğŸª `bias-variance/` | Bias-Variance Tradeoff | Understanding model complexity and generalization |
| ğŸš« `overfitting-underfitting/` | Overfitting & Underfitting | Diagnosing and addressing model performance issues |
| ğŸ›¡ï¸ `regularization/` | Regularization Techniques | L1/L2 regularization, preventing overfitting |
| ğŸ“Š `model-evaluation/` | Model Evaluation | Metrics, cross-validation, and performance assessment |

## ğŸ—ºï¸ Learning Path

We recommend following this order:

1. **supervised-vs-unsupervised/** - Understand the different ML paradigms
2. **linear-regression/** - Start with the simplest regression algorithm
3. **logistic-regression/** - Move to classification problems
4. **knn/** - Learn instance-based learning methods
5. **naive-bayes/** - Understand probabilistic classification
6. **feature-engineering/** - Learn how to prepare data effectively
7. **bias-variance/** - Understand the fundamental tradeoff in ML
8. **overfitting-underfitting/** - Learn to diagnose model issues
9. **regularization/** - Techniques to improve generalization
10. **model-evaluation/** - Master how to properly assess models

However, feel free to jump to specific topics based on your needs!

## ğŸ”‘ What You'll Learn

### ğŸ¯ Supervised Learning
- **Regression**: Linear regression for continuous predictions
- **Classification**: Logistic regression, KNN, Naive Bayes for categorical predictions
- **Model Comparison**: When to use which algorithm

### ğŸ”¬ Model Development
- **Feature Engineering**: Creating meaningful features from raw data
- **Regularization**: L1 (Lasso), L2 (Ridge), and Elastic Net
- **Hyperparameter Tuning**: Optimizing model performance

### ğŸ“Š Model Evaluation
- **Evaluation Metrics**: Accuracy, precision, recall, F1, RMSE, RÂ²
- **Cross-Validation**: K-fold, stratified, time-series splits
- **Bias-Variance Tradeoff**: Understanding model complexity

## ğŸ“‹ Prerequisites

Before starting this module, you should be familiar with:
- **Linear Algebra**: Vectors, matrices, matrix operations
- **Calculus**: Derivatives, gradients, optimization
- **Probability & Statistics**: Distributions, statistical inference
- **Python/NumPy/Pandas**: Basic data manipulation
- Completed **[01-foundations/](../01-foundations/)** or equivalent knowledge

## ğŸ¬ Getting Started

1. Ensure you have completed the prerequisites
2. Install required libraries: `pip install numpy pandas matplotlib scikit-learn jupyter`
3. Start with the recommended learning path or jump to a specific topic
4. Work through theory, code examples, and exercises in each subdirectory
5. Apply your knowledge through hands-on projects

## ğŸ“š How to Use This Module

Each subdirectory contains:
- **Theory**: Concept explanations and mathematical foundations (`.md` files)
- **Jupyter Notebooks**: Interactive code examples with visualizations (`.ipynb` files)
- **HTML Content**: Web-based interactive tutorials and explanations (`.html` files)
- **Images**: Diagrams, plots, and visual explanations (`images/` folder)
- **Exercises**: Practice problems to reinforce your learning
- **Projects**: Hands-on projects to apply what you've learned
- **Resources**: Additional reading materials and references

### ğŸ”§ Working with Jupyter Notebooks

To run the interactive examples:
```bash
# Start Jupyter Notebook
jupyter notebook

# Or use JupyterLab for a better experience
jupyter lab
```

Each notebook includes:
- ğŸ“ Step-by-step code explanations with inline comments
- ğŸ“Š Interactive visualizations and plots
- ğŸ§ª Runnable examples you can modify and experiment with
- ğŸ’ª Practice exercises embedded in the notebook
- ğŸ¯ Real-world applications and use cases

## ğŸš€ Next Steps

Once you've mastered core ML concepts, move on to:
- **[03-classical-algorithms/](../03-classical-algorithms/)** - Advanced classical ML methods (trees, SVM, clustering)

## ğŸ’¬ Contributing

Found an error? Have a suggestion? Feel free to open an issue or submit a pull request!

## ğŸ“œ License

This project is part of OpenMLAtlas - An open-source machine learning learning resource.

---

âœ¨ **Remember**: These are the fundamental building blocks of machine learning. Understanding them deeply will help you grasp more advanced concepts later!
